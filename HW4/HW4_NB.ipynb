{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7025bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5064651",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayes\n",
    "# Folder Path\n",
    "path = \"languageID\"\n",
    "languages = ['e', 'j', 's']\n",
    "\n",
    "# Read text File\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        contents = f.read()\n",
    "    return contents\n",
    "# iterate through all training files\n",
    "global_count_dict = {'e':{}, 'j':{}, 's': {}}\n",
    "for language in languages:\n",
    "    dict = {}\n",
    "    for file in glob.glob(f\"{path}/{language}[0-9].txt\"):\n",
    "        file_path = f\"{file}\"\n",
    "        contents = read_text_file(file_path)\n",
    "        \n",
    "        for char in contents:\n",
    "            if char == \"\\n\":\n",
    "                continue\n",
    "            else:\n",
    "                if dict.get(char) == None:\n",
    "                    dict[char] = 1\n",
    "                else:\n",
    "                    dict[char] = dict[char] + 1   \n",
    "    global_count_dict[language] = dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ddd88f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For language : e, the CCP vector is {' ': 0.1792499586981662, 'a': 0.0601685114819098, 'b': 0.011134974392863043, 'c': 0.021509995043779945, 'd': 0.021972575582355856, 'e': 0.1053692383941847, 'f': 0.018932760614571286, 'g': 0.017478936064761277, 'h': 0.047216256401784236, 'i': 0.055410540227986124, 'j': 0.001420783082768875, 'k': 0.0037336857756484387, 'l': 0.028977366595076822, 'm': 0.020518751032545846, 'n': 0.057921691723112505, 'o': 0.06446390219725756, 'p': 0.01675202378985627, 'q': 0.0005617049396993227, 'r': 0.053824549810011564, 's': 0.06618205848339666, 't': 0.08012555757475633, 'u': 0.026664463902197257, 'v': 0.009284652238559392, 'w': 0.015496448042293078, 'x': 0.001156451346439782, 'y': 0.013844374690236246, 'z': 0.0006277878737815959}\n",
      "\n",
      "For language : j, the CCP vector is {' ': 0.12344945665466997, 'a': 0.1317656102589189, 'b': 0.010866906600510151, 'c': 0.005485866033054963, 'd': 0.01722631818022992, 'e': 0.06020475907613823, 'f': 0.003878542227191726, 'g': 0.014011670568503443, 'h': 0.03176211607673224, 'i': 0.09703343932352633, 'j': 0.0023411020650616725, 'k': 0.05740941332681086, 'l': 0.001432614696530277, 'm': 0.03979873510604843, 'n': 0.05671057688947902, 'o': 0.09116321324993885, 'p': 0.0008735455466648031, 'q': 0.00010482546559977637, 'r': 0.04280373178657535, 's': 0.0421747789929767, 't': 0.056990111464411755, 'u': 0.07061742199238269, 'v': 0.0002445927530661449, 'w': 0.01974212935462455, 'y': 0.01415143785596981, 'z': 0.00772214263251686}\n",
      "\n",
      "For language : s, the CCP vector is {' ': 0.16826493170115014, 'a': 0.10456045141993771, 'b': 0.008232863618143134, 'c': 0.03752582405722919, 'd': 0.039745922111559924, 'e': 0.1138108599796491, 'f': 0.00860287996053159, 'g': 0.0071844839813758445, 'h': 0.0045327001942585795, 'i': 0.049859702136844375, 'j': 0.006629459467793161, 'k': 0.0002775122567913416, 'l': 0.052943171656748174, 'm': 0.02580863988159477, 'n': 0.054176559464709693, 'o': 0.07249236841293824, 'p': 0.02426690512164287, 'q': 0.007677839104560451, 'r': 0.05929511886774999, 's': 0.06577040485954797, 't': 0.03561407295488884, 'u': 0.03370232185254849, 'v': 0.00588942678301625, 'w': 9.250408559711388e-05, 'x': 0.0024976103111220747, 'y': 0.007862847275754679, 'z': 0.0026826184823163022}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "global_ccp = {'e':{}, 'j':{}, 's': {}}\n",
    "for language in languages:\n",
    "    ccp = {}\n",
    "    total = 0\n",
    "    for char in sorted(global_count_dict[language].keys()):\n",
    "        total = total + global_count_dict[language][char]\n",
    "    for char in sorted(global_count_dict[language].keys()):\n",
    "        if ccp.get(char) == None:\n",
    "            ccp[char] =  float(global_count_dict[language][char] + 0.5)/ (total + (27 * 0.5))\n",
    "    global_ccp[language] = ccp\n",
    "\n",
    "    print(f\"For language : {language}, the CCP vector is {ccp}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20cf3274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' ', 498), ('a', 164), ('b', 32), ('c', 53), ('d', 57), ('e', 311), ('f', 55), ('g', 51), ('h', 140), ('i', 140), ('j', 3), ('k', 6), ('l', 85), ('m', 64), ('n', 139), ('o', 182), ('p', 53), ('q', 3), ('r', 141), ('s', 186), ('t', 225), ('u', 65), ('v', 31), ('w', 47), ('x', 4), ('y', 38), ('z', 2)]\n"
     ]
    }
   ],
   "source": [
    "test_file = f\"{path}/e10.txt\"\n",
    "\n",
    "x_vector = {}\n",
    "for char in read_text_file(test_file):\n",
    "    if char == \"\\n\":\n",
    "        continue\n",
    "    else:\n",
    "        if x_vector.get(char) == None:\n",
    "            x_vector[char] = 1\n",
    "        else:\n",
    "            x_vector[char] = x_vector[char] + 1   \n",
    "print(sorted(x_vector.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e8a72b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log(p_hat) for e : -7841.865447060635\n",
      "Log(p_hat) for j : -8749.114299535931\n",
      "Log(p_hat) for s : -8467.28204401056\n"
     ]
    }
   ],
   "source": [
    "log_likelihood =  {'e': float(0), 'j': float(0), 's': float(0)}\n",
    "for language in languages:\n",
    "    ccp = global_ccp[language]\n",
    "    logsum = 0\n",
    "    for char in x_vector:\n",
    "        if ccp.get(char) == None:\n",
    "            ccp[char] = 0.5 / 27*0.5\n",
    "        logsum = logsum + math.log(ccp[char]) * x_vector[char]\n",
    "    print(f\"Log(p_hat) for {language} : {logsum}\")\n",
    "    log_likelihood[language] = logsum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27b7abb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log(posterior) for e : -2613.9551490202116\n",
      "Log(posterior) for j : -2916.3714331786437\n",
      "Log(posterior) for s : -2822.42734800352\n"
     ]
    }
   ],
   "source": [
    "# The prior is the same for each class as the number of samples (10) is the same.\n",
    "prior = float((10 + 0.5)) / (30 + 3*0.5)\n",
    "\n",
    "# Calculate posterior using Bayes rule\n",
    "posterior = [log_likelihood[i] * prior for i in log_likelihood]\n",
    "for i in range(3):\n",
    "    print(f\"Log(posterior) for {languages[i]} : {posterior[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d401b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfc7476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "# Layer details for the neural network\n",
    "input_size = 784\n",
    "hidden_layer1_size = 300\n",
    "hidden_layer2_size = 300\n",
    "output_size = 10\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "\n",
    "trainset = datasets.MNIST('./dataset/MNIST/', download=True, train=True, transform=transform)\n",
    "valset = datasets.MNIST('./dataset/MNIST/', download=True, train=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1552e060",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralModel():\n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exps = np.exp(x)\n",
    "        denom = np.sum(exps, axis=1)\n",
    "        denom.resize(exps.shape[0], 1)\n",
    "        return exps / denom\n",
    "\n",
    "    def __init__(self, sizes, epochs=20, l_rate=0.01):\n",
    "        self.sizes = sizes\n",
    "        self.epochs = epochs\n",
    "        self.l_rate = l_rate\n",
    "        self.init_params()\n",
    "        \n",
    "    def init_params(self):\n",
    "        input_layer = int(self.sizes[0])\n",
    "        hidden_1 = int(self.sizes[1])\n",
    "        hidden_2 = int(self.sizes[1])\n",
    "        output_layer = int(self.sizes[2])\n",
    "\n",
    "        # Random initialization of weights between -1 and 1\n",
    "        self.w1 = np.random.uniform(low=-1, high=1, size=(input_layer, hidden_1))\n",
    "        self.w2 = np.random.uniform(low=-1, high=1, size=(hidden_1, hidden_2))\n",
    "        self.w3 = np.random.uniform(low=-1, high=1, size=(hidden_2, output_layer))\n",
    "        \n",
    "        # Zero initialization of weights\n",
    "        #self.w1 = np.zeros((input_layer, hidden_1))\n",
    "        #self.w2 = np.zeros((hidden_1, hidden_2))\n",
    "        #self.w3 = np.zeros((hidden_2, output_layer))\n",
    "        \n",
    "        # Random initialization of weights from normal distribution\n",
    "        #self.w1 = np.random.randn(input_layer, hidden_1)\n",
    "        #self.w2 = np.random.randn(hidden_1, hidden_2)\n",
    "        #self.w3 = np.random.randn(hidden_2, output_layer)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # Input layer to hidden layer1\n",
    "        inputs = inputs.numpy()\n",
    "        self.linear_1 = inputs.dot(self.w1)\n",
    "        self.out1 = self.sigmoid(self.linear_1)\n",
    "        \n",
    "        # hidden layer 1 to 2\n",
    "        self.linear_2 = self.out1.dot(self.w2)\n",
    "        self.out2 = self.sigmoid(self.linear_2)\n",
    "        \n",
    "        # Hidden layer to softmax layer\n",
    "        self.linear3 = self.out2.dot(self.w3)\n",
    "        self.out3 = self.softmax(self.linear3)\n",
    "\n",
    "        return self.out3\n",
    "    \n",
    "    def backward(self, x_train, y_train, output):\n",
    "        # Convert tensors to numpy arrays\n",
    "        x_train = x_train.numpy()\n",
    "        y_train = y_train.numpy()\n",
    "\n",
    "        batch_size = y_train.shape[0]\n",
    "\n",
    "        # Derivative of loss \n",
    "        d_loss = output - y_train\n",
    "        # Calculating delta for W3\n",
    "        change_w3 = (1./batch_size) * np.matmul(self.out2.T, d_loss)\n",
    "\n",
    "        # Backpropagating to the 2nd layer from the third layer\n",
    "        d_out_2 = np.matmul(d_loss, self.w3.T)\n",
    "        d_linear_2 = d_out_2 * self.sigmoid(self.linear_2) * (1 - self.sigmoid(self.linear_2))\n",
    "        # Calculating delta for W2\n",
    "        change_w2 = (1. / batch_size) * np.matmul(self.out1.T, d_linear_2)\n",
    "        \n",
    "        # Backpropagating to the 1nd layer from the second layer\n",
    "        d_out_1 = np.matmul(d_loss, self.w3.T) * self.sigmoid(self.linear_2) * (1 - self.sigmoid(self.linear_2))\n",
    "        d_out_1 = np.matmul(d_out_1, self.w2.T)\n",
    "        d_linear_1 = d_out_1 * self.sigmoid(self.linear_1) * (1 - self.sigmoid(self.linear_1))\n",
    "        # Calculating delta for W2\n",
    "        change_w1 = (1. / batch_size) * np.matmul(x_train.T, d_linear_1)\n",
    "\n",
    "        return change_w1, change_w2, change_w3\n",
    "    \n",
    "    def update_weights(self, w1_update, w2_update,w3_update):\n",
    "        self.w1 -= self.l_rate * w1_update\n",
    "        self.w2 -= self.l_rate * w2_update\n",
    "        self.w3 -= self.l_rate * w3_update\n",
    "    def compute_loss(self, y, y_hat):\n",
    "        batch_size = y.shape[0]\n",
    "        y = y.numpy()\n",
    "        # Computing the cross entropy loss for the model and its given predictions\n",
    "        loss = np.sum(np.multiply(y, np.log(y_hat)))\n",
    "        loss = -(1./batch_size) * loss\n",
    "        return loss\n",
    "    def compute_metrics(self, val_loader):\n",
    "        losses = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, data in enumerate(val_loader):\n",
    "            x, y = data\n",
    "            # Converting to expected one-hot format\n",
    "            y_onehot = torch.zeros(y.shape[0], 10)\n",
    "            y_onehot[range(y_onehot.shape[0]), y]=1\n",
    "            # Flattening input image into 1-D\n",
    "            flattened_input = x.view(-1, 28*28)\n",
    "            output = self.forward(flattened_input)\n",
    "            predicted = np.argmax(output, axis=1)\n",
    "            # Calculating correctly predicted labels\n",
    "            correct += np.sum((predicted==y.numpy()))\n",
    "            total += y.shape[0]\n",
    "            # Computing the cross entropy loss\n",
    "            loss = self.compute_loss(y_onehot, output)\n",
    "            losses.append(loss)\n",
    "        # Performing mean over all minibatches\n",
    "        return (correct/total), np.mean(np.array(losses))\n",
    "\n",
    "    def train(self, train_loader, val_loader):\n",
    "        start_time = time.time()\n",
    "        global losses\n",
    "        global accuracies\n",
    "        for iteration in range(self.epochs):\n",
    "            for i, data in enumerate(train_loader):\n",
    "                x, y = data\n",
    "                # Since the model is producing a softmax probability over 10 classes, the label needs to be converted to a one-hot encoded vector\n",
    "                y_onehot = torch.zeros(y.shape[0], 10)\n",
    "                y_onehot[range(y_onehot.shape[0]), y]=1\n",
    "                # Converting 28x28 image into a flattened input\n",
    "                flattened_input = x.view(-1, 28*28)\n",
    "                # Forward pass the input through the model\n",
    "                output = self.forward(flattened_input)\n",
    "                # Compute gradients for the linear layer weights using SGD\n",
    "                w1_update, w2_update, w3_update = self.backward(flattened_input, y_onehot, output)\n",
    "                # Perform weight update for the minibatch\n",
    "                self.update_weights(w1_update, w2_update, w3_update)\n",
    "            # Compute the mean loss over the test set after the completion of epoch\n",
    "            accuracy, loss = self.compute_metrics(val_loader)\n",
    "            losses.append(loss)\n",
    "            accuracies.append(accuracy)\n",
    "            print('Epoch: {0}, Time Spent: {1:.2f}s, Accuracy: {2:.2f}%, Loss: {3:.2f}'.format(\n",
    "                iteration+1, time.time() - start_time, accuracy*100, loss\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b552f3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralModel(sizes=[784, 300, 10], epochs=20)\n",
    "# Training the model over the MNIST dataset\n",
    "model.train(train_loader=trainloader, val_loader=valloader)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadd13b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "# Layer details for the neural network\n",
    "input_size = 784\n",
    "hidden_size = 300\n",
    "output_size = 10\n",
    "\n",
    "# Build a simple 2-layer feed forward network as described\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_size, bias=False),\n",
    "                      nn.Sigmoid(),\n",
    "                      nn.Sigmoid(),\n",
    "                      nn.Linear(hidden_size, output_size, bias=False),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "print(model)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.0)\n",
    "# Using the cross entropy (or NLL) loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "epochs = 20\n",
    "losses = []\n",
    "for i in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        #This is where the model learns by backpropagating\n",
    "        loss.backward()\n",
    "        \n",
    "        #And optimizes its weights here\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        losses.append(float(running_loss/len(trainloader)))\n",
    "        print(\"Epoch {0}, Training loss: {1}\".format(i, running_loss/len(trainloader)))\n",
    "\n",
    "correct_count, all_count = 0, 0\n",
    "for images,labels in valloader:\n",
    "    for i in range(len(labels)):\n",
    "        img = images[i].view(1, 28*28)\n",
    "        # Turn off gradients for forward pass\n",
    "        with torch.no_grad():\n",
    "            logps = model(img)\n",
    "\n",
    "    # Output of the network are log-probabilities, need to take exponential for probabilities\n",
    "    ps = torch.exp(logps)\n",
    "    probab = list(ps.numpy()[0])\n",
    "    pred_label = probab.index(max(probab))\n",
    "    true_label = labels.numpy()[i]\n",
    "    if(true_label == pred_label):\n",
    "        correct_count += 1\n",
    "    all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(\"\\nModel Accuracy =\", (correct_count/all_count))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc74b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
